# ğŸ“ Student Performance Prediction
*End-to-end regression pipeline with PCA, From-Scratch Algorithms, Boosting, SVM, and Stacking*

This project predicts a student's **Performance Index** using a full machine learning workflow.  
The goal is to demonstrate **deep ML understanding**, not deployment â€” including:

## ğŸ“˜ Project Highlights

- End-to-end ML workflow with clean modular structure  
- PCA-based dimensionality reduction  
- From-scratch ML implementations for deeper understanding  
- Multiple sklearn models for comparison  
- Boosting and stacking ensembles  
- Strong result evaluation with RÂ², MSE, and visual analysis  
- Organized outputs: results, plots, predictions, models, processed data  

## ğŸ“‘ Table of Contents
- [Project Highlights](#-project-highlights)
- [Dataset](#-dataset)
- [Algorithms Implemented](#-algorithms-implemented)
- [Project Structure](#-project-structure)
- [Visualizations Included](#-visualizations-included)
- [Results Summary](#-results-summary)
- [How to Run the Project](#-How-to-Run-the-Project)
- [Key Learnings Demonstrated](#-key-learnings-demonstrated)
- [Future Work](#-future-work)
- [Author](#-author)


---

## ğŸ“˜ Dataset
**Student Performance Dataset**  
ğŸ”— https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression

**Target Variable:** `Performance_Index`

---

## ğŸ§  Algorithms Implemented

<div style="display: flex; gap: 40px;">

<div style="flex: 1;">

### ğŸŸ¦ From Scratch  
- Linear Regression  
- k-Nearest Neighbors  
- Support Vector Regression  
- Manual Stacking  

</div>

<div style="flex: 1;">

### ğŸŸ© Using Sklearn  
- Linear Regression  
- Ridge & Lasso  
- KNN Regressor  
- Decision Tree  
- SVR (RBF Kernel)  
- AdaBoost  
- Gradient Boosting  
- Stacking Regressor  

</div>

</div>


---

## ğŸ“‚ Project Structure

To keep the repository clean and easy to navigate, files are grouped into logical folders:

- ğŸ“˜ **Notebook** â†’  
  [`student_performance_ml_project.ipynb`](student-performance-ml-project/notebook/student_performance_ml_project.ipynb)
- ğŸ“Š **Results** (evaluation tables, model comparisons) â†’  
  [`results/`](student-performance-ml-project/results/)
- ğŸ“ˆ **Plots** (all visualizations: heatmap, PCA, model comparison, etc.) â†’  
  [`plots/`](student-performance-ml-project/plots/)
- ğŸ“„ **Predictions** (CSV predictions from each model) â†’  
  [`predictions/`](student-performance-ml-project/predictions/)
- ğŸ—‚ **Raw Data** (original dataset split into features & target) â†’  
  [`data_raw/`](student-performance-ml-project/data_raw/)
- ğŸ§® **Processed Data** (scaled data, PCA outputs, train/test splits) â†’  
  [`data_processed/`](student-performance-ml-project/data_processed/)
- ğŸ”§ **Models & Encoders** (saved scaler and label encoder .pkl files) â†’  
  [`models/`](student-performance-ml-project/models/)
- ğŸ“¦ **All Outputs ZIP** â†’  
  [`student_performance_project_outputs.zip`](student-performance-ml-project/student_performance_project_outputs.zip)


---

## ğŸ“ˆ Visualizations Included

- Heatmap of correlations  
- PCA explained variance plot  
- Actual vs Predicted (best model)  
- Model comparison bar chart  
- Error distribution plot  

All available inside the `plots/` folder.

---
## ğŸ“Š Results Summary

After evaluating all models across MSE and RÂ² metrics, the **Stacking Regressor (Sklearn)** delivered the best overall performance.

### â­ Best Model: Stacking Regressor (Sklearn)
- **Highest RÂ² Score** (~0.90 depending on run)
- **Lowest Mean Squared Error**
- **Most stable and consistent predictions**

### ğŸ” Key Insights from Model Comparison
- **Boosting models** (Gradient Boosting, AdaBoost) performed significantly better than single weak learners.
- **Regularized Linear Models** (Ridge & Lasso) showed improvement over standard Linear Regression.
- **From-Scratch Models** (kNN, Linear Regression, SVR) closely matched sklearn performance, validating correctness.
- **PCA** reduced dimensionality while maintaining predictive power, improving model stability.

Detailed metrics for all models are available in the [`results/`](results/) directory.  
Visual comparisons (bar charts, error distributions, predictions vs actual) are in [`plots/`](plots/).

See:  
ğŸ“„ `final_model_summary_with_rank.csv`  
ğŸ“Š `plot_model_comparison.png`

---
## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Set up the environment
Create and activate a virtual environment (recommended):

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Mac / Linux
source .venv/bin/activate
```
install all requiered dependencies:
```bash
pip install -r requirements.txt
```
### 2ï¸âƒ£ Run the notebook
```bash
jupyter notebook notebook/student_performance_ml_project.ipynb
```
### 3ï¸âƒ£ Project File Paths (auto-recognized)

The notebook automatically loads and saves files in the following folders:

- **data_raw/** â€” raw dataset  
- **data_processed/** â€” scaled data, PCA outputs  
- **plots/** â€” all visualizations and graphs  
- **predictions/** â€” model prediction CSVs  
- **results/** â€” evaluation metrics, comparison tables  
- **models/** â€” encoders, scaler, and saved preprocessing objects  

No manual path changes are required.

### 4ï¸âƒ£ (Optional) Reproduce Everything Automatically

If you have **papermill** installed, you can regenerate all results, plots, and outputs with a single command:

```bash
./run_all.sh
```
This will create a fully executed notebook at: 
```bash
notebook/executed_student_performance.ipynb 
```

---

### 5ï¸âƒ£ (Optional) Run in Google Colab

1. Open Google Colab  
2. Upload the main notebook: **notebook/student_performance_ml_project.ipynb**  
3. Upload the **data_raw/** folder (or mount Google Drive)  
4. Run all cells  

All other folders (`data_processed`, `plots`, `models`, etc.) will be created automatically by the notebook.



## ğŸ§  Key Learnings Demonstrated

<div style="display: flex; gap: 40px;">

<div style="flex: 1;">

### ğŸ”¹ Data Processing & Preparation  
- Categorical encoding  
- Scaling and normalization  
- PCA dimensionality reduction  
- Clean handling of training/testing sets  
- Organized saving of processed data  

### ğŸ”¹ From-Scratch ML  
- Linear Regression using matrix algebra  
- kNN using distance computation  
- SVR with simplified gradient updates  
- Manual stacking using meta-learners  

</div>

<div style="flex: 1;">

### ğŸ”¹ Model Training & Evaluation  
- Regression models (Linear, Ridge, Lasso)  
- Tree-based and boosting models  
- SVM with RBF kernel  
- Ensemble stacking (sklearn + manual)  
- RÂ², MSE evaluation metrics  
- Error distribution & prediction analysis  
- Final model ranking & comparison  

</div>

</div>
  

This project reflects depth of understanding, not just model usage.

## ğŸš€ Future Work

Here are several extensions planned for the next iteration:

- Hyperparameter tuning with GridSearchCV / Optuna  
- Adding Random Forest / XGBoost / LightGBM  
- Feature importance analysis using SHAP  
- Cross-validation pipelines  
- Outlier detection and data quality checks  
- Interactive dashboard using Streamlit  
- Model deployment (FastAPI + Docker)  

## ğŸ‘¤ Author

**Ankush Patil**  
ğŸ“ India  

ğŸ“§ **Email:** ankpatil1203@gmail.com  
ğŸ”— **LinkedIn:** https://www.linkedin.com/in/ankush-patil-48989739a  
ğŸ™ **GitHub:** https://github.com/Ankush-Patil99  

Feel free to reach out for collaborations or suggestions.
